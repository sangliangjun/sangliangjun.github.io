<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="大模型“神仙打架”，掀起复现潮、技术大升级后，我们需要关注什么？ | 万有引力, ZejunCao&#39;Blogs">
    <meta name="description" content="大模型“神仙打架”，掀起复现潮、技术大升级后，我们需要关注什么？ | 万有引力

仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接

作者|万有引力出品|CSDN（ID：CSDNnews）在过去短短的几周里，大模型赛道的信息">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>大模型“神仙打架”，掀起复现潮、技术大升级后，我们需要关注什么？ | 万有引力 | ZejunCao&#39;Blogs</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">ZejunCao&#39;Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">ZejunCao&#39;Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/frontcover/1000002572_2247586350_2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">大模型“神仙打架”，掀起复现潮、技术大升级后，我们需要关注什么？ | 万有引力</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/AI%E7%A7%91%E6%8A%80%E5%A4%A7%E6%9C%AC%E8%90%A5/">
                                <span class="chip bg-color">AI科技大本营</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-25
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/dbsJO7oaSwrdYk2t_Fy-aQ">大模型“神仙打架”，掀起复现潮、技术大升级后，我们需要关注什么？ | 万有引力</a></p>
<blockquote>
<p>仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接</p>
</blockquote>
<p>作者|万有引力<br>出品|CSDN（ID：CSDNnews）<br>在过去短短的几周里，大模型赛道的信息密度飙升至前所未有的高度。DeepSeek连续五天开源，直接引发了一场复现热潮；阿里巴巴通义实验室、腾讯相继推出面向视觉文档的RAG系统ViDoRAG、新一代混元快思考模型TurboS，加速了大模型的演进步伐；马斯克用20万张GPU训练出的Grok3，超越了许多业界标杆，再次验证了“大力出奇迹”的定律；Claude3.7Sonnet迎来编码能力大升级，AI编程的技术平权时代正在加速到来；DeepSeek论文与Kimi“撞车”，越来越多公司开始布局稀疏注意力与线性注意力机制，这些技术正成为Transformer之后的关键探索方向；此外，Manus模式的“虚拟机”概念迅速走红，正在重塑大模型的运行方式…<br>在这场眼花缭乱的技术竞赛背后，真正值得我们关注的是什么？DeepSeek的五连发究竟意欲何为？在545%的成本利润率之下，其他大模型公司是否也能找到盈利空间？面对行业变局，哪些趋势正在悄然浮现？<br>带着这些问题，CSDN特别策划的《万有引力》栏目邀请了智源人工智能研究院数据研究组负责人刘广、JinaAI创始人及CEO肖涵，在栏目主理人CSDN&amp;《新程序员》执行总编唐小引主持下，共同拆解这场大模型新战况，深入解析技术趋势、行业格局及潜在变量。<br>观点抢先看：<br>刘广：<br>无论是训练大模型还是优化推理效率，软硬件协同的创新才是关键。<br>“百模大战”的结局，目前尚无定论，但行业已取得里程碑式的进展。下一步，竞争可能会向多模态和具身智能演进。<br>AI的发展可能会沿着两条路径分化，一是少数顶级企业继续堆砌算力，追求极致的模型；大多数企业则选择小模型路线，可能通过知识蒸馏将大模型的能力压缩到小模型中，使其变得越来越强大。<br>当前Attention机制的核心问题在于算法复杂度太高了，直接导致模型的训练成本昂贵，推理时显存占用极大。<br>代码模型其实还有个很有意思的研究方向——它可能成为通往AGI的一条路径。<br>肖涵：<br>一家公司的竞争优势决定了能否在行业中赚钱，最重要的是上游价格和下游支付意愿。<br>私有化并非万能方案，除非在技术或业务层面有足够的创新支撑，否则贸然推进可能得不偿失。<br>在不改变模型的大小、尺寸、参数量的情况下，随着训练技术的精准性提升，以及高质量数据的不断积累，我们可以在有限的参数下极大地提高训练效率。<br>编程工具中，如Devin这类的AutoPilot工具更有可能成为未来主流，它特别适合“万事开头难”的场景。<br>大模型的发展可能会让擅长虚拟机技术的人火起来。因为人类文明的发展离不开工具，而代码模型正在学习如何使用工具、调用工具。工具调用必然是沙盒化的，不可能直接在本机运行。它需要能够快速启动一个轻量级的Linux内核，在虚拟机中执行任务，并在完成后输出结果。而虚拟机本身正是一个天然的沙盒环境。<br>大模型诸神之战中值得关注的点<br>唐小引：过去一段时间，AI大模型领域可谓“神仙打架”——DeepSeek开源周五连发、OpenAI发布GPT-4.5、Anthropic推出新版Claude…行业动态密集更新。在这场信息轰炸中，两位老师有何感受？能否为大家提炼几个关键词？<br>刘广：第一个关键词是DeepSeek。自春节期间DeepSeekR1的发布在全球范围内引发热议，当时不少技术人本想趁假期休息，却被DeepSeek技术的爆火吸引，都想深入探究其成功背后的关键。于是，这场热潮席卷整个AI大模型领域，各界都忙碌起来，纷纷关注并积极讨论DeepSeek的出圈现象。<br>肖涵：1月20日DeepSeekR1的发布确实是一个关键节点。它能迅速走红，我认为主要有几个原因：<br>第一，它对标的是OpenAI的推理型模型o1。实际上，推理模型在2024年上半年并不受关注，直到OpenAI在9月发布o1-preview版本，才引发广泛讨论。当时，业界流传着这样一种观点——“ScalingLaw已经走到了尽头，测试时计算（TestTimeCompute）才是新时代的关键。”包括OpenAI前联合创始人IlyaSutskever此前在接受路透社采访时表示：“实际上，ScalingLaw已经达到了极限，就快走到头了！”同时，OpenAIo1的核心贡献者NoamBrown也强调，测试时计算将带来更大的价值。<br>然而，OpenAI始终未公开o1的技术细节，市场上虽然普遍认为推理可能是未来的发展方向，但没有人能真正复现这一思路。直到DeepSeek推出R1，并完整公开其推理思维链（ChainOfThought，CoT），众人才能自己亲手尝试。<br>第二，R1让大家第一次见到模型能以类似人类的方式思考——它采用第一人称表达自己的推理过程，比如“我听到用户在问什么，我该怎么办”，展现出模型的“内心独白”。如果说此前的生成式模型更像是Seq2Seq（SequencetoSequence，序列到序列）架构，用户难以察觉模型的思考过程，那么R1的开源则让全球开发者意识到推理模型不仅可行，而且极具潜力。<br>春节期间，DeepSeek的影响力迅速扩散，对我们公司也带来一定影响，尤其是在搜索领域。要知道推理与搜索紧密相连，DeepSeek的推理架构为搜索体验带来了颠覆性变化。春节前后，Google、OpenAI、Perplexity、xAI相继推出“DeepResearch”产品，我们公司JinaAI也推出了DeepSearch，腾讯、百度也发布了各自的深度搜索产品。多种搜索产品落地的背后是源于这种推理架构。过往传统搜索往往是一次性查询，而结合推理后，系统可以“边搜边想”，持续优化结果。这颠覆了过去“速度优先”的搜索逻辑，甚至改变了用户体验。<br>1月20日DeepSeekR1的发布成为一个分水岭，此前和此后，AI搜索已是两个时代。过去，如果一个搜索系统需要三分钟才出结果，别人会觉得你别干这行转行算了。而如今，推理已成为标配，每个UI、Chat界面都开始先展示模型的思考过程，再呈现最终答案。<br>唐小引：经典的《思考，快与慢》一书中曾提及两个概念——System1和System2，指的是“快思考”和“慢思考”模式。当慢思考逐渐成为主流推理方式时，快思考的作用又是什么？<br>刘广：学术界对“快思考”和“慢思考”有不同的研究方向。从数据角度来看，传统的多模态数据（如感知类数据）通常被归为快思考，而文本类数据在大模型的预训练和微调阶段此前并不会显性展示出完整的思考链条。然而，从OpenAIo1到DeepSeekR1，这类模型开始明确展示推理链条和“内心独白”，标志着这一领域的重要转变。<br>这一变化对数据处理提出了更高要求。我们需要找到更好的方法，使数据能够更直观地展现思考链条。这并不是简单地靠强化学习微调几千条数据就能实现的。有研究表明，快思考和慢思考的能力其实在基础模型（Base模型）的训练阶段就已经部分形成，强化学习只是进一步激发它们。如果Base模型本身能力不足，后续的优化很难让它真正具备推理能力。<br>当前一些公司可能会在预训练阶段直接增强这种能力，但并非所有公司都具备这样的技术实力。另一种思路是基于已有的Base模型，将其作为快思考模型，并通过后续优化逐步激发其慢思考能力。目前，许多研究团队都在尝试复现这一过程，例如，斯坦福大学的李飞飞团队与华盛顿大学研究人员仅用50美元的成本，就训练出了一个名为s1的推理模型。<br>这些复现工作具有重要的研究意义，不过要让模型具备真正强大的泛化能力，仍然需要大规模的算力支持和精细微调，例如全参数微调R1需要使用192张GPU甚至更多，这对大多数研究团队来说成本过高，难以承受。<br>因此，目前这项技术仍处于迭代阶段，大部分研究只能在小模型上进行探索。<br>“复现DeepSeek”热潮下的技术探索与实践<br>唐小引：智源也在复现DeepSeek，有哪些观察和进展可以与我们分享？</p>
<p>目前DeepSeek并未公开数据以及整个训练的代码，所以我们希望借助开源社区的力量，从数据和系统两个角度对DeepSeek进行复现，推动DeepSeek下一代模型的研发，这也是OpenSeek的目标。OpenSeek已于2月14日正式启动，也欢迎大家一起加入共创这一项目。<br>唐小引：复现过程中遇到哪些挑战？<br>刘广：其实整个复现的过程就是“踩坑”的过程。我们的团队已经吸引了100多位贡献者，大家对模型训练的细节或多或少有所了解，但在数据处理、训练框架等方面仍然缺乏系统认知。项目起步时，我们进行了大量前期分析和讲解。<br>从数据层面来看，我们处理了约100亿网页数据，对所有样本进行了过滤、去重、质量评分（每个样本标注3-4个质量分），并进行人工抽样检查，以确保数据质量。同时，我们还采用数据合成方式，使数据更能展现推理逻辑。基于此，我们还构建了一套基于Agent的系统，复现人类的思维逻辑和思维链CoT，最终构造了约4亿条数据样本，这是目前规模最大的CoT数据集之一。<br>这些数据集的构建消耗了大量算力资源，即便是大厂也很难轻易投入如此规模的机器计算能力。这是一个极大的挑战，我们也需要说服团队投入如此大规模的资源。不过，我们计划在后续全面开源这些数据集，为整个开源社区提供支持。<br>唐小引：这样成本很高，但实际上很多团队能够用很低的成本完成复现？<br>刘广：正常复现只需要一个基础模型，比如基于阿里的Qwen2.5，这样成本相对较低，但倘若想要打造出超越Qwen2.5、DeepSeekV3更强的基础模型，成本就会上升。我们在每个阶段都希望有更好的版本，因此如果能开源整个过程、数据和框架，实际上能推动技术向前发展。这一点很重要，因为目前很多中间环节是黑盒状态，若完全公开，大家就能在此基础上改进。<br>此外，在系统训练层面，当前训练DeepSeekV3这样的大规模模型缺乏Megatron（NVIDIA出品）等成熟的框架支持，如何高效训练DeepSeekV3成为一大难题。不久前，DeepSeek公开了自研的五个项目，我们能够基于此进一步提升训练效率。此外，我们团队未来还计划支持多种芯片，拓展至英伟达生态之外，使这一技术更具广泛意义。<br>从这两个方面来看，OpenSeek项目值得关注。<br>唐小引：相比DeepSeek，OpenSeek最大的区别在于不局限于英伟达的生态？<br>刘广：对，我们的目标是打破单一依赖，比如在单节点训练时不局限于英伟达的GPU，而是探索其他芯片的可行性。<br>唐小引：这种方式意义更大。接下来，肖老师团队的复现工作进展如何，是复现DeepResearch吗？<br>肖涵：是的，我们公司在2023-2024年间主要专注于搜索领域的基础模型研发，如Embedding和Reranker。其中，我们的Embedding和Reranker在HuggingFace上的下载量位居前列，每月下载量达200万次。自2023年中后期起，我们也开始探索搜索领域的垂直小模型，重点关注2B参数以下的模型。<br>在搜索领域，小模型主要用于数据清理，例如从HTML提取适合大模型处理的Markdown、纯文本或XML格式。虽然这一任务可以通过正则表达式或大模型完成，但正则表达式在多语言支持和内容理解方面存在局限，而大模型成本高昂，并且需要超长的上下文窗口（通常至少百万级token）。<br>为此，我们推出了两个专门用于网页数据清理的模型：ReaderLM（2024年7月发布）和ReaderLM-v2（2025年1月发布）。ReaderLM-v2在多语言支持方面有所优化，基于Qwen微调，并针对长文本和HTML到纯文本的转换进行了改进。这些模型在海外社区引发了广泛关注。<br>随着DeepSeek的发布，我们起初开始思考是否需要一个专门用于搜索推理的小模型。其实DeepSearch并没有使用到类似DeepSeekR1或OpenAIo1中的ThinkingModel，而是主要依赖AgenticSearch，相当于上文中提到的“快思考”，其工作方式类似状态机，包含网页抓取、内容读取、推理、反思等多个环节，并在这些状态之间循环运行。<br>普通推理模型与搜索推理模型的关键区别在于，搜索推理模型需要在推理过程中频繁打断，以调用外部工具，并将结果重新输入思维链。这是DeepSeekR1等正常推理无法做到的，除非在训练时进行特殊优化。<br>因此，如果要训练一个搜索推理模型，首先需要构建迭代式数据集，即如何在输入、输出之间插入合理的思维链，这涉及合成数据生成（SyntheticData）。事实上，当前的循环式DeepSearch本质上也可以看作一个数据标注器，它通过显式规则和用户反馈生成数据。<br>未来的搜索竞争点可能不再是Embedding或Reranker，而是搜索策略（SearchStrategy）。关键在于如何在不同阶段选择合适的搜索策略，以提高信息获取的精准度，这也是我们最近思考的重点。<br>唐小引：在具体场景下，我用Windsurf调Claude3.7模型，希望实现无代码开发。它会调用各种工具进行安装和部署，但如果报错，它会持续尝试，而身为用户的我只能等待。进而错误不断出现后，只能手动叫停。即便如此，它仍然不断报错，却无法自行解决问题。这是什么原因？<br>肖涵：这体现出当前模型会穷举式地尝试各种方案。这正是我认为Agentic机制不可或缺的原因。推理模型无法完全独立迭代，某些情况下需要外部干预，比如人为介入，或者另一个Agent来中断它，做出这样的提示：“你已经错了三次，换个思路吧。”<br>因此，我会在程序里加入规则，比如“如果错误超过三次，就触发特定的Prompt。”有些人觉得这种方法很“土”，但目前推理模型的局限性就在于它无法自主跳出固定思维模式。<br>此外，工具调用本身会削弱模型的推理能力。GoogleGemini的文档也提到过这一点，因为模型会倾向于直接调用工具，而不是自主推理。这种情况在搜索时尤为明显——模型如何判断自己是否该调用工具？<br>其实对于1+1&#x3D;2这种简单问题，大模型可以直接回答，但面对复杂数学题，它可能需要生成Python代码并执行计算。关键在于，模型如何决策：是依赖自身知识，还是调用外部工具？这一问题仍然没有完美的解决方案。<br>刘广：这确实是大模型领域的一大争议点：模型是否知道自己不知道？它在何时确信自己的答案是正确的？何时会产生幻觉或不确定性？这些问题直接影响工具调用的合理性和决策能力。<br>在特定领域，Agent系统会调用工具来执行任务。但从更深层次来看，我们希望模型能更精准地判断自身的知识边界，提高确定性，减少幻觉。这不仅涉及工具调用的优化，也关乎模型的核心能力。为此，建立评测体系至关重要，比如衡量幻觉率、置信度等关键指标。智源也在深入研究这一方向，通过不同任务对大模型的整体表现进行评估，以推动更可靠、更可控的AI发展。<br>唐小引：这是否应该从模型训练、开发阶段就去解决问题？<br>刘广：实际上，模型开发本质上是围绕评测目标展开的。如果要优化代码能力，就必须设定针对性的评测标准，否则开发方向容易发散。因此，各大模型团队都会建立内部评测体系，设定关键指标，例如降低幻觉率，以确保模型的可靠性。<br>降低幻觉率的关键在于数据优化。首先，要确保数据的质量和真实性；其次，可以通过技术手段分析模型的置信度，例如判断某个token是否可能引发幻觉。目前，已有研究探索如何量化模型在回答时的置信度，并结合多种机制提升准确性。<br>在OpenSeek项目中，我们也在不断优化工具调用和代码能力。如今，大模型在单一问题回答和推理任务上的表现已经相当优秀，尤其是在数学和代码方面的能力提升明显。但像Claude3.7这样在代码能力上表现出色的模型，其本质上是在弥补大模型的不确定性，大模型本质上是概率模型，难以保证完全确定的输出。因此，代码能力和工具调用的优化，或许正是未来大模型发展的关键方向。<br>唐小引：这意味着先“卷”程序员？<br>刘广：没错，先“卷”程序员。我们的目标不是单纯提升某一方面，而是在整体均衡的基础上，对特定能力进行重点优化。如果我们更看重代码能力，就会强化代码数据和训练比重，但其他能力也不能落下，关键在于找到平衡点。<br>从数据角度来看，预训练数据几乎已被充分利用，比如网页数据大约十几TB，代码数据主要来自GitHub或其他渠道，来源相对固定。因此，如何更高效地利用这些数据，甚至生成合成数据，是当前研究的一个方向。<br>正如肖总上文提及的，数据可以通过Agentic方法优化推理过程，使其更易被模型理解。我们在OpenSeek也会探索这方面的尝试，提升数据质量。目前，代码领域尤为缺乏高质量、人工校验或系统校验的数据，因此仍有很大的优化空间。<br>DeepSeek开源周的五连发意味着什么？<br>唐小引：DeepSeek开源周的核心在于公开DeepSeekAIInfra的关键技术，主要涉及三大领域：<br>计算：FlashMLA、DeepGEMM（算力加速与硬件优化）<br>通信：DeepEP、DualPipe、EPLB（优化通信与并行计算）<br>存储：3FS、Smallpond（存储与数据处理）<br>DeepSeek为什么选择开源这些技术？它的真实意图是什么？这将如何影响AIInfra领域？<br>刘广：提前预告一下，这五个开源仓库未来都将集成到OpenSeek，方便大家进行模型训练和推理。<br>从内容上看，这些项目与云厂商的工作类似，主要围绕计算、通信、存储进行优化——云厂商和硬件厂商的核心任务也是提升这三大模块的效率，以支持云端计算、存储和数据传输。通过这些算子，AI服务能力得以提升，本质上也是在优化云厂商和硬件厂商的效率。<br>当然，这些项目也存在局限性。目前，它们主要针对英伟达H系列（Hooper系列）硬件进行了优化，尚未覆盖其他芯片。<br>从更广的角度来看，这不仅是OpenSeek的机会，也是整个行业的机会。任何人都可以在此基础上进行优化，甚至通过开源合作，在不同硬件上实现类似优化，降低开发成本，减少重复劳动。<br>对于存储、算力和通信有着很多需求的硬件厂商而言，这种趋势可能带来更大的竞争压力，而模型训练团队则可以直接利用这些开源项目，降低开发成本，加快迭代速度。当然，当前底层Infra仍有很大优化空间。<br>此外，值得注意的是，现在行业里Infra团队和人才相对稀缺，目前行业内更多在做从0到1的搭建，而从1到100的优化相对较少。因此，未来如果有更多长期项目推动Infra领域的深度优化，将会非常重要。例如，智源的FlagScale就是一个值得关注的项目。<br>肖涵：我认为DeepSeek发布的系统级开源项目意义重大，尤其是在其模型取得成功后，更能证明其技术的可信度。许多系统框架的创新往往是为了解决特定问题，但如果缺乏成功的实际案例，很容易被忽视。<br>DeepSeek的框架创新不仅有成功的模型作为背书，而且在推理速度优化和量化方面展现出了领先优势。相比之下，其他公司在这一领域仍相对薄弱，而DeepSeek的投入显然更深入、更系统化。<br>唐小引：从你的角度来看，DeepSeek为什么要在英伟达生态中做极致优化？是否考虑过其他生态？<br>肖涵：我认为其中一个原因是英伟达的文档非常完整。许多开发者曾反映，其他硬件平台的文档不够深入，迁移到其他平台时，文档中往往缺少关键信息。英伟达的CUDA文档非常详细，许多人深挖这一层内容，这对做极致优化非常有帮助。<br>刘广：软硬件结合并不是新概念，OpenAI早已提出，并与英伟达展开深度合作。A100和H100服务器在早期都是优先供给OpenAI，而OpenAI开发Triton编译器，本质上也是为算法创新提供系统级优化。<br>DeepSeek也在走类似的路径，他们吸纳了英伟达的人才，在GitHub仓库的贡献者中，也能看到与英伟达的关联。正是这种深厚的底层技术积累，让DeepSeek在软硬件协同方面具备了改进空间，并能进行系统化的工程设计。<br>因此，无论是训练大模型还是优化推理效率，软硬件协同的创新才是关键。<br>唐小引：像DeepSeek等模型现如今依然选择了PyTorch、TensorFlow等框架？<br>肖涵：当前大模型主要依赖PyTorch，谷歌也在推广JAX，但使用者相对较少。而DeepSeek开源周给我最大的触动在于，它不仅专注于AI模型的创新，更在系统层面进行了深度优化，特别是在底层架构上。这种稳扎稳打的做法令人印象深刻。<br>许多人对DeepSeek的开源感到震惊，因为它与OpenAI、Anthropic等专注于产品级创新的路径不同，而是更加聚焦于系统基础层面的突破。这也是DeepSeek与其他AI公司的关键区别之一。<br>唐小引：很多人称DeepSeek为“源神”，是当之无愧的开源典范。你刚才提到，DeepSeek完全颠覆了你对AI公司的印象，能谈谈你之前对AI公司的印象吗？<br>肖涵：在我的刻板印象里，许多AI公司往往把50%资金投向营销，另一半用于研发，甚至研发投入可能更少。但DeepSeek走了一条完全不同的路，始终专注于技术深耕，并保持低调务实的风格。<br>更关键的是，DeepSeek能否长期坚持这一理念，继续保持“静水深流”的发展模式？如果无法坚持下去，我们或许只能转向OpenSeek这样的开源项目了。<br>唐小引：国内是否有能与DeepSeek匹敌的新兴公司？DeepSeek掀起的开源浪潮中，阿里的Qwen也具有不小的影响力，但为什么这次的焦点不是Qwen？是什么因素导致这一差异？新兴公司是否有机会在这一赛道中挑战DeepSeek？<br>肖涵：我一直很喜欢Qwen模型，尤其是在长文本支持和指令跟随方面表现出色。Qwen2.5在训练EmbeddingReader等任务上效果很好。2024年，Qwen在国际市场上也有不俗表现，特别是在社区互动和国际影响力方面，成功跨越地缘政治障碍，吸引了众多海外粉丝。<br>尽管Qwen在外网知名度很高，但近期未能占据行业焦点，可能与其技术路线选择有关。相比DeepSeek聚焦推理和推理优化，Qwen更倾向于多模态和代码模型的开发，因此在推理方面未能取得R1级别的突破。不过，无论是Qwen还是DeepSeek，都是中国AI领域的重要代表，展现了强劲的技术实力。<br>唐小引：那新兴公司是否有能与之匹敌的呢？<br>肖涵：新兴公司的日子，可谓艰难（笑）。<br>刘广：我注意到一个有趣的趋势，许多量化公司开始转型成立AI部门，九坤就是一个典型例子。这类公司在系统优化和数据处理方面积累深厚，它们具备软硬件协同开发的能力，尤其在处理海量异质数据时具有明显优势。<br>然而，是否能像DeepSeek一样打造有影响力的开源项目和高效模型，仍是一个未知数。我希望这次行业不会重蹈“百团大战”的覆辙，而是能找到各自的差异化发展路径，形成更加良性的竞争格局。<br>唐小引：大家都说自己是AI公司。<br>刘广：是的，但记住80&#x2F;20原则，市场上最终只会记住老大和老二，像可口可乐和百事可乐，其他品牌几乎没人记得。<br>肖涵：对于新兴公司来说，AI基础设施、大模型训练，尤其是通用大模型，确实需要非常大的投入。要么走纯开源路线，比如DeepSeek，专注技术创新；要么走完全封闭的系统，比如OpenAI、Anthropic，做到顶尖水平。<br>除此之外，很难找到中庸的平衡，而且新兴公司要专注于应用层的创新，因为只有这样才能在大模型发展的浪潮中脱颖而出，不至于被淹没。<br>唐小引：百模大战至今，算是结束了吗？<br>肖涵：我觉得第一阶段暂时是DeepSeek领先，接下来就看有没有后来居上者了。<br>刘广：从AGI或更广义的人工智能发展路径来看，目前仍处于早期阶段。真正实现AGI可能需要经历多个阶段，而当前的R1主要是文本推理模型，未来的发展方向将逐步扩展至多模态和具身智能。<br>关于“百模大战”的结局，目前尚无定论，但行业已取得里程碑式的进展。接下来，竞争可能会向多模态和具身智能演进，就像具身智能领域已出现类似“百身大战”的格局，多模态方向同样涌现出众多探索者。<br>目前，文本模型已基本达到R1级别，下一步能否突破仍需观察。一部分人将继续推动技术前沿，而另一部分则专注于基于现有模型开发下游应用，进一步拓展AI生态版图。<br>肖涵：应用确实是检验模型落地能力的关键。我们不希望模型在技术上不断突破、各种系统级创新层出不穷之际，却缺乏真正赚钱的应用。<br>如果应用端能盈利，比如代码生成、短视频生成这些领域，企业赚到钱后，才会进一步反哺模型研发，推动多模态能力的增强。这种良性循环才能真正带动整个生态发展，否则就违背了基本的经济规律。<br>基于大模型的MaaS服务到底挣不挣钱？<br>唐小引：DeepSeek开源五连发后还带来了一个“OneMoreThing”，即发表了《DeepSeek-V3&#x2F;R1推理系统概念》的文章，披露了两点信息：一是通过架构优化提升模型推理的吞吐量并降低延迟，二是测算了大规模部署的成本与收益，得出的数据颠覆了许多人的认知——DeepSeek的利润率竟高达545%。<br>这也让大模型MaaS（模型即服务）是否具备盈利能力成为讨论焦点。在“MaaS在中国短期内可能是最糟糕的商业模式”和“MaaS能否成功关键在于技术实力和用户基础”的讨论中，大家真正关心的是大模型商业化的问题。AIInfra真的只是亏损生意吗？为什么外部部署DeepSeek可能亏钱，而DeepSeek自己做推理却能盈利？你们怎么看？<br>肖涵：首先，一家公司的竞争优势决定了能否在行业中赚钱，最重要的是上游价格和下游支付意愿。其中，你的上游是哪个云服务器，是否能以低价获取GPU，这直接影响你能定什么价格。其次，下游客户愿意付多少钱，这也不完全由我们决定，市场竞争决定了价格水平。<br>可以说，MaaS的最大问题在于其壁垒非常脆弱。上游不给低价，客户又不愿付高价，就容易陷入两难境地。在这种情况下，唯一的壁垒就是精简运营，做到最小化，一切都要紧凑高效，不容有任何浪费。<br>唐小引：首先得保证活下去？<br>肖涵：不仅是活下去，而是要确保组织内部的运营效率，这是唯一的壁垒。对于MaaS公司来说，面对上游无法获得理想价格、下游客户不愿支付的情况，唯一的解决办法就是优化运营。<br>唐小引：那这个生意很难做。<br>肖涵：确实很难做。市场也会遵循80&#x2F;20法则。如果模型可以更快速地下载到本地或减少延迟，这对国内客户有吸引力。但最终市场仍会由一两家巨头主导，DeepSeek目前可能已经占据了这个位置。<br>唐小引：做模型有护城河吗？<br>肖涵：护城河是相对且短暂的。SamAltman曾经坚信OpenAI自己有坚固的护城河，并警告其他公司不要与之竞争。但事实证明，OpenAI的护城河也并不坚不可摧。相反，像Anthropic通过差异化发展，在代码能力上找到了一条成功之路。虽然Claude是通用模型，但其90%的用户主要用它来生成代码，因此他们不断强化这方面的能力，最终让Claude成为编程领域的领先工具。<br>刘广：从应用角度来看，之前我们主要从2C角度讨论应用，但2B领域的应用影响同样巨大。我们曾与一些企业合作，发现许多公司希望进行私有化部署和定制化模型，比如预训练或微调，但往往面临一个核心问题——数据并不适合直接用于模型训练。<br>虽然企业掌握着海量数据，比如PDF文件、截图、照片等，但这些数据通常无法直接用于训练，而很多企业并不清楚什么样的数据才合适。因此，国外有不少公司专门提供数据治理和处理服务，以优化模型训练。在2B领域，对数据治理和数据处理的需求非常旺盛，国内也在加速推进这方面的建设，比如国家成立数据局，推动数据要素市场的发展。<br>从市场角度来看，私有化部署的需求正在快速增长，未来谁能解决数据处理和治理问题，谁就能在这一市场占据先机。<br>唐小引：如果专攻2B领域走私有化部署的路线，但数据无法直接利用，这条路该怎么走？<br>刘广：许多公司确实对如何利用大模型感到困惑，也不清楚数据该如何处理。目前，一些大型互联网公司可能已经掌握了完整的流程，但对于许多传统企业来说，这仍然是一个陌生领域，尤其是在数据处理和模型私有化部署方面，仍然需要大量的知识普及。<br>如今，DeepSeek的出现进一步冲击了这一市场。许多企业开始认为，直接使用DeepSeek可能就能满足需求，甚至无需额外的微调和训练。<br>唐小引：DeepSeek能满足2B领域的需求吗？<br>肖涵：最近与国内一些客户交流时，我发现DeepSeek一经推出，许多公司迅速将原有模型替换，甚至将“上DeepSeek”作为业务指标，成为领导层汇报的重要内容之一。<br>对于私有化部署，我建议企业将其作为长期目标，而非短期任务。若一开始就急于推进私有化部署，往往会陷入复杂的技术和管理困境，导致项目延误数月。私有化部署涉及设备采购、内部网络环境配置等诸多问题，尤其对外部B2B销售而言，客户的内部IT环境具有高度不确定性，增加了实施难度。<br>因此，在验证可行性时，建议优先使用API，即MaaS服务。DeepSeek提供的API适用于快速原型验证，无需微调，只需将数据输入上下文窗口，通过Prompt进行学习。在这一过程中，企业可以明确问题所在——是模型生成质量欠佳，还是成本过高，或是其他因素。如果这些问题无法通过私有化部署有效解决，那么就需要重新评估其必要性。私有化并非万能方案，除非在技术或业务层面有足够的创新支撑，否则贸然推进可能得不偿失。<br>现实来看，私有化部署确实是一个迫切需求。企业往往希望数据严格留在内部网络，不经过外网传输。然而，现在是否就该推进DeepSeekR1的完全私有化部署？我认为，考虑到技术更新速度极快，这一决策需要谨慎。比如，未来OpenSeek可能会成为更优选择，届时你可能会发现当前部署的版本已不再适用，导致资源浪费。<br>与此同时，私有化部署还面临诸多挑战。首先，企业需要构建支持多租户的系统，以满足不同部门的计算需求，并妥善管理token统计等问题。目前虽然可以搭建一个综合系统，但前提是企业需投入高昂成本购置A100等硬件。其次，如何在这些设备上高效运行最先进的推理技术也是一大难题。当前许多厂商开始推出软硬一体机，打包硬件与优化后的软件一起销售，以降低部署和运维的复杂度。<br>唐小引：这会成为未来的主要方向吗？还是只是昙花一现？<br>肖涵：目前市面上有很多AIPC，购买电脑时，DeepSeek等软件可能会捆绑一起。<br>唐小引：听起来有些像是泡沫？<br>肖涵：这可以看作是一种趋势，但能否长期持续还有待观察。总的来看，DeepSeek公开了模型推理成本和利润的细节，我们也在探讨API接口是否会推动价格下降。技术层面，MoE（专家混合）是否会成为行业的必选方案？这一点仍然存在不确定性。<br>刘广：模型训练成本将逐步降低。目前，行业的重点是数据收集和大规模数据集的构建，并在验证过程中进行模型训练。随着训练成本下降，中小企业将有更多机会负担得起模型训练费用，促进技术的普及。过去，高昂的训练成本使许多中小企业无法进行预训练，但一旦门槛降低，他们将能够加入竞争，从而推动整个生态进入良性循环。<br>唐小引：DeepSeek的开源透明度能帮助中小企业吗？<br>刘广：目前或许还不行，但这确实是一个值得期待的趋势。未来随着训练效率的提升和技术进步，成本将大幅下降，而类似“面壁miniCPM”这样的轻量级模型也会不断优化，效果越来越好。<br>肖涵：企业在应用大模型时，除了训练成本，还需考虑试错成本和人才成本。并非所有企业都有能力复现DeepSeek，关键在于人才储备。DeepSeek的招聘标准极高，而市场上具备预训练能力的人才本就稀缺。许多博士生的工作重点往往是微调，而非从零搭建预训练模型。因此，真正能够主导预训练的专家屈指可数，这也使得大规模自主研发难度更大。<br>唐小引：但此前IlyaSutskever很明确地提出了“预训练时代已经结束”。<br>肖涵：预训练的核心在于追求顶尖效果，普通的预训练模型往往缺乏竞争力，与其投入资源训练平庸的模型，不如等待别人推出更先进的开源模型。因此，尽管有人认为微调已过时，但在特定场景下，它仍然不可或缺，尤其是针对高度私有或结构化的数据。例如，通用模型在公有数据上的表现优异，但面对企业的专有数据，往往难以达到理想效果，此时微调便成为提升模型适用性的关键手段。<br>唐小引：DeepSeek流行后，有些人建议不要急于做微调。那领域问题如何解决？<br>刘广：领域模型的挑战在于数据准备。比如医疗、金融行业，数据的隐私保护非常严格，导致领域数据无法流通，这使得领域模型很难进展。<br>我们提出了“DataAgent”概念，利用大模型的能力构建自动化的数据处理系统，帮助从领域内挖掘数据并转换为训练数据。当前我们正在推进这一构想，并计划在今年下半年取得一些成果。<br>肖涵：领域数据的微调并非只是将数据灌入模型，关键在于数据配比。比如，金融领域的数据与通识数据的比例很重要。我们使用合成数据来实现精细化配比，这种方法对小模型的应用尤为重要，能够帮助我们更好地发挥模型的潜力。<br>大力出奇迹的大模型法则依然成立还是已走向终点？</p>
<p>然而，仅仅十天后，OpenAI发布了史上规模最大的AI模型GPT-4.5，舆论风向却截然不同。许多人质疑它成本高昂，但基准测试的提升并不显著，甚至显得平平无奇。于是，关于AI发展的核心命题再次浮现：“大力出奇迹”这条路究竟还能走多远？还是已经逼近极限？<br>刘广：我认为ScalingLaw仍然有效。从结果来看，在投入大量算力、数据和资金后，模型能力的确有了提升，正如Grok3的表现确实比DeepSeekV3更强。如果这些资源投入后，模型没有明显提升，那才表明ScalingLaw可能失效了。<br>至于Grok3，其算力投入确实巨大。尽管马斯克曾提到要开源，但目前我们还不了解其具体的模型架构。如果继续沿用Grok-1的思路，极有可能是类似Mistral架构的大型MoE（混合专家）模型，且每个Expert的规模可能非常大。而DeepSeek的策略则侧重小粒度Expert，强调效率。因此，从成本和算力利用率的角度来看，DeepSeek的方案可能更为高效。Grok3显然没有考虑这些优化，依靠庞大的算力走的是“暴力美学”的路线。这种方式的优势在于，充足的算力使得模型可以快速试验和迭代，模型的规模不断扩大，能力也确实得到提升。他们可能会继续沿着这条路探索，直到达到极限。<br>OpenAI的情况可能有所不同。他们的目标似乎是打造“情商模型”，更加注重情感理解、陪伴交互，甚至更细致的情感分析。因此，GPT-4.5可能只是一个过渡版本，或者他们的整体方向与我们的假设有所不同。<br>至于OpenAI是否已经遇到了ScalingLaw的瓶颈，目前尚不清楚。如果真的遇到了瓶颈，他们可能会重新思考效率、模型结构、数据等方面的问题，寻找新的突破。数据问题尤其关键。高质量的数据是有限的，合成数据是否能够有效支持ScalingLaw的延续？据说GPT-4o的大量训练数据为合成数据，这可能有助于解决问题，但数据的多样性仍然是一个挑战。如果数据问题突破，且数据规模能够无限增长，那么ScalingLaw或许能够继续保持有效。<br>除外之外，对于算力的扩展，这在一定程度上是可预见的。只要新芯片不断推出，网络规模不断扩大，从20万张GPU扩展到200万、2000万，甚至2亿张，理论上ScalingLaw仍然成立。但问题在于，成本会极为高昂。到那个阶段，只有少数资源雄厚的企业才能承担得起。<br>因此，未来可能会出现两种发展路线：<br>1.	少数顶级企业继续堆砌算力，追求极致的模型。<br>2.	大多数企业则选择小模型路线，可能通过知识蒸馏将大模型的能力压缩到小模型中，使其变得越来越强大。<br>最终，AI的发展可能会沿着这两条路径分化。<br>肖涵：站在做小模型的角度来看，我一直认为小模型的能力会不断提升，而大模型的能力一定会遇到瓶颈。大模型之所以会遇到瓶颈，不是因为无法继续扩展或提升，而是因为ROI（投资回报率）变得非常低，投入大量成本，最后的提升可能只是几百分点，这种游戏不是所有公司都能承受得起。<br>相反，像测试时间计算（testtimecompute）和推理时间（inferencetime）等方面，并不一定有业界说的那么好，而是因为大家之前过于关注预训练和ScalingLaw，忽视了推理时间和测试时间的潜力。事实上，针对这些领域，有很多现成的、简单可得的成果，轻松就能收获大回报。对于我们这种小公司来说，我们更务实，关注的并不是大模型的ScalingLaw是否会“撞墙”，而是更专注于提升小模型本身的性能。<br>如果用今天的3B模型与三年前的3B模型相比，很多人会发现，今天的3B模型相当于当时的11B模型，这是为什么？<br>我总是用内燃机汽车的发展历程作为比喻。现在的小米SU7Ultra已经有1000多马力，而几十年前，这种动力可能难以想象。回看1920年代，宝马就已经开始制造八缸发动机，并把它放到车里。虽然现在的车比过去要大一些，但并没有夸张到无法上路，发动机舱依旧差不多大，而今天，我们能够在这些舱内放进更强劲的发动机。<br>这其实和模型的发展很相似：在不改变模型的大小、尺寸、参数量的情况下，随着训练技术的精准性提升，梯度优化、EarlyStop、Grokking技术、BudgetForcing等手段的提升，以及高质量数据的不断积累，我们可以在有限的参数下极大地提高训练效率。这一直是我们坚信的方向。我们不考虑大于2B模型的扩展，而是专注于2B以下的模型，探索如何在特定领域和任务中比如搜索、推理、重排、数据清理等方面达到优秀的效果。这是我们公司对ScalingLaw的理解。甚至我们还发布了一个关于Embedding模型的ScalingLaw图，非常骄傲地分享了出来，结果发现没什么人关注。这就是小公司常有的现象，慢慢就习惯了。<br>唐小引：在众多大模型中，GoogleGemini似乎显得较为低调？<br>肖涵：我们一直在使用Gemini2.0Flash版本，体验非常不错，速度极快。在企业端API领域，它的价格极具竞争力，甚至让人觉得它在大厂中的定位有些类似于DeepSeek——价格相对亲民，同时在速度和准确性方面表现优秀，精准地抓住了几个关键点。<br>许多人可能忽视了大模型的一个重要需求——结构化输出。除了代码生成、图像生成、诗歌创作和深度思考等功能外，市场对JSON格式输出的需求已经成为刚需。如果某个大模型能在这方面做到极致，那无疑是一条突围之路。而Flash版本在这方面表现出色，价格合理，速度也很快。<br>此外，Google大模型有多个版本，比如GoogleAIStudio和VertexAI，二者在性能上有所不同。GeminiStudio版本的SLA（服务水平协议）保障率较低，偶尔会有延迟；而VertexAI版本则非常稳定，特别适合企业级应用。Google在多模态方面的投入也令人惊艳。例如，在发布会上，他们直接用摄像头录制屏幕，Gemini能够实时分析并提供操作指导。这种真正的多模态能力不仅是单向的，而是双向的，甚至可以多模态输出，展现出了强大的能力。<br>至于Gemini相对低调的原因，可能在于市场热度的分配。例如，Anthropic发布Claude3.5时，程序员社区的反应非常热烈，而OpenAI本身就拥有一批忠实粉丝。相比之下，Google虽然在持续发力，但关注度似乎没那么高。值得注意的是，Google最近挖来了HuggingFace创始团队成员之一PhilippSchmid，并让他负责Gemini的开发者关系，这表明他们正加速布局开发者生态。<br>刘广：Gemini的一大亮点在于长文档处理能力。发布时，它展示了一个令人惊艳的Demo——直接输入一个小时的视频，Gemini不仅能给出详细的描述，还能精准定位到每一帧。这种能力在多模态数据标注方面极具价值，我了解到不少团队已经在使用Gemini进行这类任务，并取得了显著效果。<br>肖涵：长窗口是Gemini早期就让人印象深刻的特性。最初它宣称支持100万token的窗口，引发热议，结果第二天又传出还有一个10Milliontoken版本尚未发布。许多人认为RAG（检索增强生成）在搜索领域已经逐渐过时，而我在进行DeepSearch研究时，确实也没有用到向量化或向量数据库。为什么？因为Gemini的窗口足够长，基本不用担心超出100万token的限制。尤其是在深度搜索时，由于是迭代式检索，新信息会不断填充窗口，随后进行总结、归纳或推理，再进一步检索。只要窗口空间足够大，基本上就能得到所需答案。<br>在长上下文（LongContext）方面，我们看到了一场技术变革，它可能彻底改变搜索领域的格局。过去，很多搜索技术栈和理念可能已经不再适用，比如Embedding的作用。在实践中，我们发现Embedding主要被用作去重工具，而非召回手段。因此，我们在思考，如果Embedding在长上下文环境下的主要用途是去重，而非搜索召回，那么优化STS（SentenceSemanticSimilarity）任务的优先级就要提高，而不是继续围绕异构搜索展开优化。<br>唐小引：有哪些技术栈已经过时了？<br>肖涵：不能说完全过时，但有些变化是很明显的。<br>首先，长窗口（longcontext）是必不可少的。过去很多人觉得它只是个实验性特性，但现在我们发现，从端到端的应用来看，它确实很有用。<br>其次，QueryRewrite变得越来越重要。其实QueryRewrite并不新鲜，早在三四十年前的NLP研究中就有人提出。但过去的QueryRewrite只是简单的关键词转换，而现在我们可以用大模型蒸馏小模型来进行重写。具体核心是挖掘用户的深层意图，甚至是他们自己没有意识到的需求。我们需要提炼出这些隐藏意图，即使用户明确表示“不想看某些内容”，但如果对他们有帮助，我们仍然要把这些内容呈现给他们。QueryRewrite的关键就是确保搜索结果真正符合用户的潜在需求，而不仅仅是表面需求。<br>唐小引：你们为什么要做这些改进？<br>肖涵：因为我们在做DeepSearch。传统的关键词搜索已经无法满足用户需求，大家需要更深入的内容，类似DeepSeek追求的“AHAMoment”（顿悟时刻）。所以，我们必须把Query这一环做得更精准、更深度。<br>至于向量数据库，我不过多评论，它可能还是有用的。</p>
<p>但是，当DeepSeekR1这样的推理模型出现后，RAG范式就被颠覆了。这些框架需要在保留RAG兼容性的同时，快速适应新的推理搜索范式，但这个过程会很慢。因此，我认为当前搜索领域的新范式是“带推理的深搜索”（DeepSearchwithReasoning），在这个模式下，传统的RAG并不那么高效。<br>唐小引：Claude3.7和ClaudeCode都是基于AgenticSearch，没有用RAG，之前炙手可热的技术栈现在有哪些被刷新了？<br>肖涵：我们内部也在思考这个问题。<br>首先，Embedding和Reranking这些模型的用途可能和我们最初设想的不同。现在，很多人直接用大模型做召回，甚至回归到关键词搜索。关键词搜索速度快，成本低，然后把结果交给大模型，由大模型来优化precision和recall。<br>甚至rerank也可以交给大模型完成，比如当你有大量URL时，下一个应该访问哪个？可以单独用rerank，也可以让大模型通过内部推理来决定。这种模式对传统搜索市场冲击很大，让许多不上不下的模型被大模型吞噬，而关键词搜索（如BM25、ElasticSearch）反而变成刚需。<br>其次，evaluation（评测）变得越来越重要，甚至可以作为系统的一部分，而不仅仅是后期评测。<br>唐小引：大模型如何评估自己的幻觉问题？<br>肖涵：对于这个问题，我尝试了两种方法。<br>方案A：在输出答案时，让模型自己生成一个confidence（置信度），告诉我们它对自己输出内容的信心有多高。<br>方案B：让另一个模型（可以是相同的模型，但prompt不同）作为评测者，专门评估前一个模型的回答是否存在幻觉。<br>我发现方案B的效果更好，即使是同一个模型，不同的角色设定就能产生不同的效果。这让我意识到evaluation这一环有很多优化空间，甚至可以结合强化学习。在test-timecompute时，我们可以让evaluator先拦截输出，检查内容是否幻觉、过时或不完整，再决定是否继续输出。这种方法被称为BudgetForcing，即在输出前强制等待片刻，反思一下答案质量，再继续生成。<br>刘广：关于长上下文的支持，现在有很多模型侧的改进，比如DeepSeek提出的NSA（NativeSparseAttention）。这种改进可以让模型在训练过程中处理超长上下文，同时训练成本降低十倍以上。<br>推理时，可能也会有类似优化，比如固定的上下文窗口，甚至可以理论上支持无限长度的上下文。<br>同时，Kimi也提出了MixtureofBlockAttention（MoBA）机制，这种机制从Attention结构本身优化长上下文的处理能力，效果可能更优。<br>Attention机制的演进</p>
<p>从这些趋势可以看出，传统的Attention机制正面临挑战。那么，当前Attention机制存在哪些问题？学术界和产业界又有哪些新的研究进展？<br>刘广：当前Attention机制的核心问题在于算法复杂度太高了，直接导致模型的训练成本昂贵，推理时显存占用极大，尤其是KVcache的占用会随着上下文长度的增加而迅速膨胀，显存需求成倍增长，计算量也随之增加。<br>因此，改进Attention机制的主要方向有两个：<br>1.让Attention计算更稀疏化<br>现在的Attention计算方式很“密集”，不管一个token重要与否，都要计算一遍，导致计算效率低下。目前常用的计算方式是，Attention计算并非均匀分配给所有token，而是高度集中在少数关键token上。例如，StreamingLM观察到，第一个token的Attention权重往往占据主导地位。这一发现启发了一种优化思路——如果仅保留Attention权重较高的token，而减少低权重token的计算量，就能在保证生成质量的同时降低计算成本，并维持一定的连贯性和一致性。<br>基于这一思路，SparseAttention主要发展出三种优化方法：<br>滑窗注意力（SlidingAttention）：通过滑窗机制，将token划分为局部窗口，仅计算相邻token之间的Attention关系，减少整体计算量。<br>选择注意力（SelectedAttention）：在超长上下文（如16Ktoken）中，仅计算部分关键token的Attention，跳过低重要性的token。<br>压缩注意力（CompressedAttention）：通过压缩方法，将整个上下文浓缩到一个固定长度的窗口中，以减少计算开销。<br>目前，NSA（NeuralSparseAttention）结合了上述方法，使计算更加高效，同时还能保持较好的生成效果。<br>2.SSM（状态空间模型）<br>由于传统Attention计算复杂度高，显存占用大，因此业界探索了如何将其计算复杂度降低到线性级别，以减少计算量并降低推理成本。但这种优化存在一个典型的trade-off：计算效率提升的同时，模型的表达能力可能会下降。</p>
<p>仅保留一个固定的KVcache（或state），作为记忆状态。<br>计算时只需更新状态，而无需像传统Attention那样计算整个序列的Attention权重。<br>由于KVcache是固定的，这种方式能有效降低训练和推理的计算成本，并提高计算效率。<br>不过，目前的研究表明，SSM仍然主要侧重于提高计算效率，其性能在某些任务上可能仍与标准Transformer存在一定差距。<br>关于NSA（NeuralSparseAttention），最让我惊讶的是，它在减少计算量的同时，模型效果反而提升了。按理说，减少计算、压缩信息通常会导致性能下降，但NSA通过选择性地保留关键token，有效降低了计算量，同时保持了模型对关键信息的建模能力。实验结果显示，loss甚至更低，推理效果更好。<br>我们OpenSeek计划尝试复现NSA方法，看看它是否真的能在降低计算成本的同时，带来更好的生成质量和推理效率。<br>唐小引：你们在复现NSA？<br>刘广：对，我们这边有一些贡献者在帮忙做这个事情，也在往这个方向努力。这其实是一个很有价值的尝试。<br>唐小引：Attention的演进方向，未来会进一步朝稀疏化发展吗？<br>刘广：目前来看，稀疏化是一种权衡（trade-off）。比如MiniMax提出的线性化Attention，虽然在理论上可行，但实际应用很难做到，它也是一种trade-off。线性化更偏向计算效率，而稀疏化则更关注效果。本质上，这像是在调整不同的阈值。完整的self-attention计算量最大，但效果最好；线性Attention计算量最小，但效果差距可能比较大；稀疏Attention介于两者之间，试图在效率和效果之间找到平衡。<br>不过，要同时做到这两点，可能还需要新的架构突破，不是短时间能解决的。<br>肖涵：从工程角度看，我们对Attention的最大期待是提升显存效率，尤其是在微调（fine-tuning）时，Attention常常是个瓶颈。<br>比如我们的小模型Reader支持512K长文本窗口，如果直接用FullAttention，显存会“爆炸”，训练时还要跟踪梯度，占用更是翻倍，这和推理阶段的需求完全不同。<br>所以，我们更希望像刘老师团队把这个问题研究透了，告诉我们哪种方式更合适，我们就直接应用，不会在Attention机制上花太多额外精力。<br>另外，像Mamba和RWKV这样的异构架构其实早已存在，尤其是RWKV，早在三四年前就已开始发展，并进入基金会孵化。我记得2022年在西班牙瓦伦西亚开会时，Linux基金会正式宣布RWKV进入孵化阶段。<br>然而，异构架构的推广取决于两个关键因素：是否能在特定硬件上发挥最大性能，以及主流社区的支持度。目前，像Mistral和Llama这样的社区活跃度很高，但它们并未采用这种架构，因此RWKV和Mamba的发展仍面临一定阻力。尽管这一方向已经探索多年，至今仍缺乏明确的信号指引未来发展路径。<br>如果回溯更早期的机器学习，稀疏化一直是重要概念。最初，计算是基于全量数据的，后来逐步引入稀疏化，例如L1、L2正则化等。早在20年前，机器学习就已在研究稀疏化，而今天看来，这一方向依然具有现实意义。<br>唐小引：Mamba和RWKV这些架构，这几年讨论也很多。它们和Transformer的竞争态势是怎样的？<br>刘广：从模型结构上看，RWKV和Mamba的优势在于流式处理，比如实时音频、视频，因为它们的KVcache占用很小，显存开销低。而Transformer结构在文本任务上仍然更强。<br>现在还有一个新方向——DiffusionLM，它的效率很高，不像Transformer那样逐步生成，而是一次性生成1000个甚至更多个token。这样一来，未来的架构可能会呈现“百花齐放”的局面，各种模型结构都有它们的应用场景。<br>肖涵：DiffusionLM真的很惊艳。比如用Claude生成代码时，它是一行一行改，改起来很慢。但DiffusionLM一次性出完整代码，然后再调整，这种体验对开发者更友好。<br>其实Diffusion相关的研究很早就有了，GoogleDeepMind之前用Diffusion生成音乐，但当时没有结合歌词。现在DiffusionLM在文本领域的应用，确实是一个很有意思的创新点。<br>唐小引：除了架构，还有哪些新方向你们觉得值得关注，但可能还没被行业足够重视？<br>刘广：数据。合成数据正在成为一个重要方向。<br>在多模态、具身智能、文本任务等领域，合成数据的应用越来越广泛。许多团队会用大模型生成数据，然后再把这些数据发布到网络上，供后续模型训练使用。现在抓取的网页数据中，可能有很大一部分其实是模型自己生成的。<br>这对未来模型也带来了一个挑战：如何区分这些合成数据，并在预训练过程中合理利用它们？<br>此外，具身智能领域也可以用模拟器合成数据，提升真实场景模拟的精度。在多模态检索任务中，合成数据能帮助训练更强的检索模型。甚至在人脸识别等分类任务上，我们也可以用Diffusion模型合成大量高质量的人脸数据。<br>相比传统的数据增强（dataaugmentation），合成数据的精准度更高，几乎是“指哪打哪”，这可能会成为未来模型训练的关键技术之一。<br>肖涵：确实，数据合成的价值已经超过了传统的数据增强。过去的数据增强是随机旋转、翻转、模糊等，现在完全可以直接生成符合需求的数据，提升数据质量和多样性。我觉得未来这块的发展会越来越快。<br>模型巨变进行时，程序员迎来怎么样的挑战与机遇？<br>唐小引：随着首个混合推理模型Claude3.7的发布，以及智能编码工具ClaudeCode的推出，AI代码能力迎来了大幅跃升。国内也有类似的尝试，如字节跳动Trae于近日在国内上线并接入了DeepSeek，海外版接入Claude。AI代码能力的提升，会加剧程序员的焦虑，还是带来新机遇？DeepSeek曾引发“技术平权”讨论，如今AI编程也展现出类似趋势——普通用户借助AI就能实现创意，不再依赖程序员。这是否会催生新的创新爆发？而在编码领域，相比DeepSeek，Claude是否更值得程序员关注？<br>肖涵：我每天使用的AI编程工具非常多，主要分为两大类：<br>第一类是Copilot类工具，比如Cursor。这类工具本质上就是在编辑器里嵌入了一个AI助手，左侧是代码编辑器（EditorWindow），右侧是AI聊天窗口（AIChatWindow）。相比手动在浏览器中使用AI编程工具生成代码再复制粘贴，Copilot类工具减少了频繁切换窗口的麻烦，让整个编程体验更加流畅。此外，像Claude或DeepSeek这样的AI生成代码时，通常会输出完整代码，而不是增量修改（diff），需要手动复制粘贴到编辑器中。而Cursor、Windsurf这类工具可以直接生成diff，点击“Accept”即可一键应用修改，几百行代码瞬间生成，大幅提升效率。我主要用Copilot来检查代码仓库，查找Bug或优化空间，在这些场景下，这类工具非常好用。<br>第二类是Autopilot。我们目前在用的是Devin——由ScottWu在纽约创办的初创公司CognitionAI开发的AI软件工程师。它的使用体验类似于在微信里给员工布置任务，对方执行后再反馈结果。区别在于，这里的“员工”是AI，而整个任务执行过程是全自动的。我观察过Devin的运行方式，它会在命令行、代码窗口和浏览器之间不断切换，并且在浏览器端具备一定的模态能力，能判断何时该点击哪个按钮，以检查前端效果。这正是Autopilot的核心——你只需要下达指令，半小时后，它就能交付一个完整的项目。<br>目前，这两类工具我都在用，但我认为AutoPilot未来更有可能成为主流，它特别适合“万事开头难”的场景——当你有想法但不知道从何下手时，AutoPilot能快速生成一个初步版本，为你提供可行的起点。当然，这类工具也有局限性，需要对多模态能力、大模型工具调用以及长文本处理的要求非常高。如果满分100分，我会给当前的AutoPilot工具打60分。它生成的项目通常能作为新功能的基础，其中60%的代码可以直接运行，但仍然需要人工检查，避免遗漏关键逻辑。从使用体验来看，AutoPilot更像是一个“能力不错的实习生”，但成本更低，每月500美元，比雇佣实习生还划算。<br>而Copilot也有其优势，比如在修复Bug方面表现不错，但在理解整个代码库时则显得力不从心。我们JinaAI的代码库规模庞大，即使针对其中一个项目，AI在解析代码、优化性能或提供深入见解时仍然容易卡住。即便使用Claude3.7，在分析到一定深度后，依然会遇到瓶颈。<br>随着AI编程能力的提升，用户的期待也大幅提高。如果说一年前，AI随便生成一个贪吃蛇游戏就足以让人惊叹，那么现在，人们更希望它能解决更复杂的代码问题，比如优化老代码、理解业务逻辑，甚至给出自己的见解，指出代码中的不合理之处。然而，在这些方面，AI仍然有待提升。<br>我一直关注阿里的通义千问团队，他们在每次发布新的编码模型时，都会用SVG矢量图生成来测试AI，比如让它画小猪、小象，以检验生成的精准度。这种测试方法非常巧妙，表面上看是代码生成，实际上考验的是AI对几何关系的理解、长文本的处理能力以及代码生成的准确性。<br>SVG代码看似简单，但涉及坐标计算，tokenizer处理后会形成很长的字符串，实际上对大模型是个不小的挑战。我也尝试让AI优化SVG，比如让Claude3.7处理JinaAI的Logo，生成一个漂亮的SVG动画，结果它折腾半天，只让球弹了一下就结束了……不管是Claude3.7还是DeepSeek，在代码能力上都还有很大的提升空间。<br>不过，尽管存在这些不足，我们依然对AI编程充满期待。毕竟，去年AI营收最后的公司都是做代码相关的，如Vercel、Cursor、Windsurf、Devin等。<br>唐小引：是的，此前Babel创始人兼CEO张海龙也曾评价过，「AI编程是第一个真正经过PMF（产品市场契合度）验证的应用场景，所以能出圈」。<br>肖涵：没错。<br>唐小引：Claude现在把编程能力作为核心升级方向，是因为AI编程更容易实现、已经验证过市场价值，还是为了做差异化竞争？<br>肖涵：我认为这完全取决于市场反馈和PMF。去年AI编程工具的爆发，几乎让Anthropic成了最大赢家，因此Claude选择专攻这个方向是有道理的。<br>说到底，有多少人会真的用大模型写诗、画画、陶冶情操？大多数用户更关心AI如何提高生产力，而编程恰恰是最直接、最具可衡量性的生产力工具。代码的好坏可以通过编译、运行、单元测试来验证，而像视频生成、文创内容的评判标准更模糊，往往需要人工干预。因此，在AI还处于发展期时，编程无疑是最现实、最能变现的突破口。<br>刘广：代码模型其实还有个很有意思的研究方向——它可能成为通往AGI的一条路径。<br>类似于具身智能领域的“终身学习”概念，代码模型也在不断习得新技能、积累经验并提升能力。在Minecraft（我的世界）这款游戏中，研究人员通过代码模型组合已有技能，形成新的能力，比如合成材料、建造房屋。一旦成功，这些技能就会被存储，逐步形成一个技能库。代码模型的成长路径也是如此——通过持续学习和进化，它能够完成的任务将越来越多。<br>随着Claude3.7这样的代码模型不断进化，代码技能库的规模也将大幅扩展。我曾与实习生探讨过一个设想：如果我们能整合全球所有代码工具，构建一个包含一千万种工具的庞大库，那么AI是否就能胜任几乎任何编程任务？如果在此基础上不断构建和拓展代码技能库，最终是否可能孕育出AGI？<br>肖涵：刘老师这个观点很好。人类文明的发展离不开工具，而代码模型正在学习如何使用工具、调用工具，以及在什么场景下调用。一旦掌握了这些，或许在AI的定义里，AGI就实现了。Anthropic推出MCP（ModelContextProtocol，模型上下文协议），其实就是各种代码工具的调用规范，目的也是让AI更好地使用工具。<br>不过，最近他们展示的“AI通关宝可梦”的案例，我觉得不太能说明问题。玩过宝可梦的人都知道，这个游戏本身很简单，通关不需要复杂的策略，战斗基本上平A就行。他们说模型学会了工具调用，但实际上，Prompt设计很简单，只是告诉它前面有个类似GameBoy的设备，可以选择按A、B、X、Y、上下左右等键，再结合多模态能力操作游戏。<br>真正的工具调用，尤其是结合编程能力的调用，远比这复杂。甚至，当现有工具不够用时，AI需要自己编写代码，把工具封装进来。这种能力，或许才是通向大模型最终智能的关键。说到这里，我觉得做虚拟机方面的人应该会很火。<br>唐小引：为什么是虚拟机？<br>肖涵：因为工具调用一定是沙盒化的，不可能直接跑在本机上。它需要能快速启动一个轻量级的Linux内核，在虚拟机里执行任务，跑完后再输出结果。虚拟机本身就是一个沙盒环境。<br>唐小引：已经开始火了，还是即将火？<br>肖涵：现在有不少人在做，类似Docker，但会比Docker更轻量化。<br>唐小引：正如肖老师提到的，大多数人对大模型的期待不是用它来写诗、画画，而是提升生产力，甚至在具身智能领域，帮人类解决那些脏活累活。那么，具身智能有什么新的进展？<br>刘广：具身智能领域正迎来机器人热潮，人形机器人成为各大企业竞相探索的方向。然而，以往的做法更偏向自动化，即通过预设编码固定机器人的动作，一旦场景发生变化，机器人往往难以适应，泛化能力较弱。<br>但今年春节期间，Figure02采用了VLA模型——即大模型与多个小模型的组合，这种架构显著提升了机器人的泛化能力，使其能够执行更多不同的操作。因此，当前业内的趋势正是通过VLA模型增强机器人在复杂环境中的适应性。<br>与此同时，国内宇树科技近期也推出了一款能跳舞的机器人，甚至登上了春晚舞台，不仅能完成360度旋转，还能表演中国功夫，精准模拟人的全身动作。这类全身运动的学习能力意味着，一旦掌握一个动作，就能泛化到更多动作。如果构建一个丰富的动作库，机器人将能不断提升泛化能力。<br>整体来看，行业的方法论正在从传统的自动化控制逐步向基于模型的方法转变，让机器人具备更强的学习与适应能力。<br>关于《万有引力》：<br>这是由CSDN&amp;《新程序员》执行总编唐小引主理的对话栏目。技术趋势多变，一不留神总担心错过。正在发生的技术事件，对于我们开发者意味着什么？我们面临的诸多困惑从何寻找答案？《万有引力》即志在于此，直面事件与困惑，抽丝剥茧，解读技术真相。<br>栏目定位：一档面向开发者群体，聚焦解读技术事件的对话直播栏目。<br>直播观看平台：CSDN视频号、CSDN网站&amp;App<br>多形式：文章、视频、音频都会有，持续关注CSDN公众号都可获取。目前《万有引力》栏目已上线小宇宙平台，欢迎大家关注！<br>推荐阅读：<br>▶阶跃星辰TechFellow段楠：Step-Video系列模型的关键技术解读</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ZejunCao</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://zejuncao.github.io/2025/03/25/1000002572-2247586350-2/">https://zejuncao.github.io/2025/03/25/1000002572-2247586350-2/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">ZejunCao</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/AI%E7%A7%91%E6%8A%80%E5%A4%A7%E6%9C%AC%E8%90%A5/">
                                    <span class="chip bg-color">AI科技大本营</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/03/25/1000002572-2247586350-1/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/1000002572_2247586350_1.jpg" class="responsive-img" alt="2025人工智能系列活动全景透视，慕尼黑上海电子展邀您共探未来！">
                        
                        <span class="card-title">2025人工智能系列活动全景透视，慕尼黑上海电子展邀您共探未来！</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AI%E7%A7%91%E6%8A%80%E5%A4%A7%E6%9C%AC%E8%90%A5/">
                        <span class="chip bg-color">AI科技大本营</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/03/25/1000000246-2247492697-1/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/1000000246_2247492697_1.jpg" class="responsive-img" alt="7.6K+ Star！PydanticAI：一个AI应用开发框架">
                        
                        <span class="card-title">7.6K+ Star！PydanticAI：一个AI应用开发框架</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AIGC%E5%88%9B%E6%83%B3%E8%80%85/">
                        <span class="chip bg-color">AIGC创想者</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <a href="/about" target="_blank">ZejunCao</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ZejunCao" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:caozejun369@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1378463428" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1378463428" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/5915009280" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/5915009280" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/Garfusion/posts" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/Garfusion/posts" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
